#!/usr/bin/env python3

"""
Tiny-yolo-v4 device side decoding demo
The code is the same as for Tiny-yolo-V3, the only difference is the blob file.
The blob was compiled following this tutorial: https://github.com/TNTWEN/OpenVINO-YOLOV4
"""

import multiprocessing as mp
import queue
import sys
import time
from pathlib import Path

import cv2
import depthai as dai
import numpy as np
import yaml

# Get argument first
nnPath = 'cameraFunc/models/yolo-v4-tiny-tf_openvino_2021.4_6shave.blob'

with open('config.yml', 'r') as stream:
    config = yaml.load(stream, Loader=yaml.FullLoader)

if not Path(nnPath).exists():
    raise FileNotFoundError(
        f'Required file/s not found, please run "{sys.executable} install_requirements.py"')

# tiny yolo v4 label texts
labelMap = [
    "person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train",
    "truck", "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench",
    "bird", "cat", "dog", "horse", "sheep", "cow", "elephant",
    "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie",
    "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat",
    "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", "wine glass", "cup",
    "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich",
    "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake",
    "chair", "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor",
    "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", "oven",
    "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors",
    "teddy bear", "hair drier", "toothbrush"
]


def runPedestrianCamera(frame_queue, command, alert):
    # Create pipeline
    pipeline = dai.Pipeline()

    # Define sources and outputs
    camRgb = pipeline.createColorCamera()
    detectionNetwork = pipeline.createYoloDetectionNetwork()
    manip = pipeline.createImageManip()

    xoutRgb = pipeline.createXLinkOut()
    nnOut = pipeline.createXLinkOut()
    manipOut = pipeline.createXLinkOut()

    xoutRgb.setStreamName("rgb")
    nnOut.setStreamName("nn")
    manipOut.setStreamName("manip")

    # Properties
    camRgb.setPreviewSize(1080, 720)
    camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
    camRgb.setInterleaved(False)
    camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
    camRgb.setFps(10)

    # Network specific settings
    detectionNetwork.setConfidenceThreshold(0.5)
    detectionNetwork.setNumClasses(80)
    detectionNetwork.setCoordinateSize(4)
    detectionNetwork.setAnchors(
        np.array([10, 14, 23, 27, 37, 58, 81, 82, 135, 169, 344, 319]))
    detectionNetwork.setAnchorMasks(
        {"side26": np.array([1, 2, 3]), "side13": np.array([3, 4, 5])})
    detectionNetwork.setIouThreshold(0.5)
    detectionNetwork.setBlobPath(nnPath)
    detectionNetwork.setNumInferenceThreads(2)
    detectionNetwork.input.setBlocking(False)

    manip.initialConfig.setResizeThumbnail(416, 416)
    manip.initialConfig.setFrameType(dai.ImgFrame.Type.BGR888p)
    manip.inputImage.setBlocking(False)

    # Linking
    # camRgb.preview.link(detectionNetwork.input)

    camRgb.preview.link(manip.inputImage)
    manip.out.link(detectionNetwork.input)

    detectionNetwork.passthrough.link(xoutRgb.input)

    detectionNetwork.out.link(nnOut.input)

    # Connect to device and start pipeline
    found, device_info = dai.Device.getDeviceByMxId(config["REAR_CAMERA_ID"])
    if not found:
        raise RuntimeError("device not found")
    device = dai.Device(pipeline, device_info)

    # Output queues will be used to get the rgb frames and nn data from the outputs defined above
    qRgb = device.getOutputQueue(name="rgb", maxSize=4, blocking=False)
    qDet = device.getOutputQueue(name="nn", maxSize=4, blocking=False)

    frame = None
    detections = []
    startTime = time.monotonic()
    counter = 0
    color2 = (255, 255, 255)

    # nn data, being the bounding box locations, are in <0..1> range - they need to be normalized with frame
    # width/height
    def frameNorm(frame, bbox):
        normVals = np.full(len(bbox), frame.shape[0])
        normVals[::2] = frame.shape[1]
        return (np.clip(np.array(bbox), 0, 1) * normVals).astype(int)

    def displayFrame(name, frame):
        color = (255, 0, 0)
        for detection in detections:
            bbox = frameNorm(
                frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
            cv2.putText(frame, labelMap[detection.label], (bbox[0] +
                                                           10, bbox[1] + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            cv2.putText(frame, f"{int(detection.confidence * 100)}%",
                        (bbox[0] + 10, bbox[1] + 40), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            cv2.rectangle(frame, (bbox[0], bbox[1]),
                          (bbox[2], bbox[3]), color, 2)
        # Show the frame
        cv2.imshow(name, frame)

    while command.value != 0:
        inRgb = qRgb.get()
        inDet = qDet.get()

        if inRgb is None or inDet is None:
            continue

        frame = inRgb.getCvFrame()

        detections = inDet.detections
        counter += 1

        color = (255, 0, 0)
        person_count = 0

        for detection in detections:

            try:
                label = labelMap[detection.label]
            except:
                label = detection.label

            if label != 'person':
                continue

            person_count += 1
            bbox = frameNorm(
                frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))
            cv2.putText(
                frame, label, (bbox[0] + 10, bbox[1] + 20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            cv2.putText(frame, f"{int(detection.confidence * 100)}%",
                        (bbox[0] + 10, bbox[1] + 40), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)
            cv2.rectangle(frame, (bbox[0], bbox[1]),
                          (bbox[2], bbox[3]), color, 2)

            # 駕駛之外有人才會警報
            if person_count <= 1:
                continue

            if alert.value == 0:
                alert.value = config["RED_ALERT_SIGNAL"]

        # crop black out of image
        frame = frame[91:325, 0:416]
        if config["PRODUCTION"] is True:
            frame = cv2.resize(frame, (config["MainImage_Width"], config["MainImage_Height"]),
                               interpolation=cv2.INTER_LINEAR)

        try:
            frame_queue.put_nowait(frame)
        except queue.Full:
            pass


def main():
    frame_queue = mp.Queue(4)
    command = mp.Value('i', 1)
    alert = mp.Value('i', 0)

    proccess = mp.Process(target=runPedestrianCamera,
                          args=(frame_queue, command, alert,))
    proccess.start()

    while True:
        try:
            frame = frame_queue.get_nowait()
            cv2.imshow('frame', frame)
        except queue.Empty or queue.Full:
            pass

        if cv2.waitKey(1) == ord('q'):
            command.value = 0
            break

    proccess.kill()


if __name__ == '__main__':
    main()
